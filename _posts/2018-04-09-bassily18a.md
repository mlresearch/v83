---
title: Learners that Use Little Information
abstract: "\r We study learning algorithms that are restricted to using a small amount
  of information from their input sample. We introduce a category of learning algorithms
  we term {\\em $d$-bit information learners}, which are algorithms whose output conveys
  at most $d$ bits of information of their input. A central theme in this work is
  that such algorithms generalize. \r We focus on the  learning capacity of these
  algorithms, and prove sample complexity bounds with tight dependencies on the confidence
  and error parameters. We also observe connections with well studied notions such
  as sample compression schemes, Occamâ€™s razor, PAC-Bayes and differential privacy.\r
  We discuss an approach that allows us to prove upper bounds on the amount of information
  that algorithms reveal about their inputs, and also provide a lower bound by showing
  a simple concept class for which every (possibly randomized) empirical risk minimizer
  must reveal a lot of information. On the other hand, we show that in the distribution-dependent
  setting every VC class has empirical risk minimizers that do not reveal a lot of
  information.\r "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bassily18a
month: 0
tex_title: Learners that Use Little Information
firstpage: 25
lastpage: 55
page: 25-55
order: 25
cycles: false
author:
- given: Raef
  family: Bassily
- given: Shay
  family: Moran
- given: Ido
  family: Nachum
- given: Jonathan
  family: Shafer
- given: Amir
  family: Yehudayoff
date: 2018-04-09
address: 
publisher: PMLR
container-title: Proceedings of Algorithmic Learning Theory
volume: '83'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 4
  - 9
pdf: http://proceedings.mlr.press/v83/bassily18a/bassily18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
