---
title: Minimax Optimal Bayes Mixtures for Memoryless Sources over Large Alphabets
abstract: The normalized maximum likelihood (NML) distribution achieves minimax log
  loss and coding regret for the multinomial model. In practice other nearly minimax
  distributions are used instead as calculating the sequential probabilities needed
  for coding and prediction takes exponential time with NML. The Bayes mixture obtained
  with the Dirichlet prior $\operatorname{Dir}(1/2, …, 1/2)$ and asymptotically minimax
  modifications of it have been widely studied in the context of large sample sizes.
  Recently there has also been interest in minimax optimal coding distributions for
  large alphabets.  We investigate Dirichlet priors that achieve minimax coding regret
  when the alphabet size $m$ is finite but large in comparison to the sample size
  $n$. We prove that a Bayes mixture with the Dirichlet prior $\operatorname{Dir}(1/3,
  …, 1/3)$ is optimal in this regime (in particular, when $m > \frac{5}{2} n + \frac{4}{n
  - 2} + \frac{3}{2}$).  The worst-case regret of the resulting distribution approaches
  the NML regret as the alphabet size grows.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jaasaari18a
month: 0
tex_title: Minimax Optimal Bayes Mixtures for Memoryless Sources over Large Alphabets
firstpage: 470
lastpage: 488
page: 470-488
order: 470
cycles: false
author:
- given: Elias
  family: Jääsaari
- given: Janne
  family: Leppä-aho
- given: Tomi
  family: Silander
- given: Teemu
  family: Roos
date: 2018-04-09
address: 
publisher: PMLR
container-title: Proceedings of Algorithmic Learning Theory
volume: '83'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 4
  - 9
pdf: http://proceedings.mlr.press/v83/jaasaari18a/jaasaari18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
