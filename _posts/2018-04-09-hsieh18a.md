---
title: Dimension-free Information Concentration via Exp-Concavity
abstract: Information concentration of probability measures have important implications
  in learning theory. Recently, it is discovered that the information content of a
  log-concave distribution concentrates around their differential entropy, albeit
  with an unpleasant dependence on the ambient dimension. In this work, we prove that
  if the potentials of the log-concave distribution are \emph{exp-concave}, which
  is a central notion for fast rates in online and statistical learning, then the
  concentration of information can be further improved to depend only on the exp-concavity
  parameter, and hence can be dimension independent.  Central to our proof is a novel
  yet simple application of the variance Brascamp-Lieb inequality. In the context
  of learning theory, concentration of information immediately implies high-probability
  results to many of the previous bounds that only hold in expectation.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hsieh18a
month: 0
tex_title: Dimension-free Information Concentration via Exp-Concavity
firstpage: 451
lastpage: 469
page: 451-469
order: 451
cycles: false
author:
- given: Ya-ping
  family: Hsieh
- given: Volkan
  family: Cevher
date: 2018-04-09
address: 
publisher: PMLR
container-title: Proceedings of Algorithmic Learning Theory
volume: '83'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 4
  - 9
pdf: http://proceedings.mlr.press/v83/hsieh18a/hsieh18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
